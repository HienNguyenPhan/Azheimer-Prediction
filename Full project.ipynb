{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48ec90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "pip install nibabel numpy scipy pandas torch torchvision sklearn matplotlib seaborn alive_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d6ce04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import zoom, shift, rotate\n",
    "import os\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from alive_progress import alive_bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b841c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "def resize_and_normalize_fMRI(data, affine, target_shape=(64, 64, 48)):\n",
    "    \"\"\"\n",
    "    Resize và chuẩn hóa cường độ cho ảnh fMRI.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Mảng 3D của ảnh fMRI\n",
    "    - affine: Ma trận affine của ảnh gốc\n",
    "    - target_shape: Kích thước mục tiêu (mặc định: 64x64x48)\n",
    "    \n",
    "    Returns:\n",
    "    - normalized_data: Mảng đã resize và chuẩn hóa\n",
    "    \"\"\"\n",
    "    # Resize ảnh về kích thước mục tiêu\n",
    "    zoom_factors = [t / s for t, s in zip(target_shape, data.shape)]\n",
    "    resized_data = zoom(data, zoom_factors, order=3)  # Nội suy tricubic\n",
    "\n",
    "    # Chuẩn hóa cường độ\n",
    "    mask = resized_data != 0\n",
    "    mean_intensity = np.mean(resized_data[mask]) if np.any(mask) else 0\n",
    "    std_intensity = np.std(resized_data[mask]) if np.any(mask) else 1\n",
    "    normalized_data = (resized_data - mean_intensity) / std_intensity\n",
    "    \n",
    "    return normalized_data\n",
    "\n",
    "def augment_fMRI(input_path, output_dir, target_shape=(64, 64, 48)):\n",
    "    \"\"\"\n",
    "    Augment ảnh fMRI bằng zoom, shift, và rotate, tạo ra 3 ảnh mới và giữ ảnh gốc.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_path: Đường dẫn đến file NIfTI đầu vào\n",
    "    - output_dir: Thư mục để lưu các file NIfTI đã augment\n",
    "    - target_shape: Kích thước mục tiêu (mặc định: 64x64x48)\n",
    "    \n",
    "    Returns:\n",
    "    - List các tên file đã tạo\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Đọc ảnh fMRI\n",
    "        img = nib.load(input_path)\n",
    "        data = img.get_fdata()\n",
    "        affine = img.affine\n",
    "\n",
    "        # Tạo tên file cơ bản\n",
    "        base_name = os.path.splitext(os.path.basename(input_path))[0]\n",
    "        if base_name.endswith('.nii'):\n",
    "            base_name = os.path.splitext(base_name)[0]  # Xử lý .nii.gz\n",
    "\n",
    "        output_files = []\n",
    "\n",
    "        # 1. Ảnh gốc\n",
    "        normalized_data = resize_and_normalize_fMRI(data, affine, target_shape)\n",
    "        output_path = os.path.join(output_dir, f\"{base_name}_original.nii.gz\")\n",
    "        nib.save(nib.Nifti1Image(normalized_data, affine), output_path)\n",
    "        output_files.append(f\"{base_name}_original.nii.gz\")\n",
    "        print(f\"Đã lưu: {output_path}\")\n",
    "\n",
    "        # 2. Zoom\n",
    "        zoom_factor = np.random.uniform(0.9, 1.1)  # Zoom ngẫu nhiên 90%-110%\n",
    "        zoomed_data = zoom(data, zoom_factor, order=3)\n",
    "        normalized_zoomed = resize_and_normalize_fMRI(zoomed_data, affine, target_shape)\n",
    "        output_path = os.path.join(output_dir, f\"{base_name}_zoomed.nii.gz\")\n",
    "        nib.save(nib.Nifti1Image(normalized_zoomed, affine), output_path)\n",
    "        output_files.append(f\"{base_name}_zoomed.nii.gz\")\n",
    "        print(f\"Đã lưu: {output_path}\")\n",
    "\n",
    "        # 3. Shift\n",
    "        shift_pixels = [np.random.uniform(-5, 5) for _ in range(3)]  # Dịch chuyển ±5 voxel\n",
    "        shifted_data = shift(data, shift_pixels, order=3)\n",
    "        normalized_shifted = resize_and_normalize_fMRI(shifted_data, affine, target_shape)\n",
    "        output_path = os.path.join(output_dir, f\"{base_name}_shifted.nii.gz\")\n",
    "        nib.save(nib.Nifti1Image(normalized_shifted, affine), output_path)\n",
    "        output_files.append(f\"{base_name}_shifted.nii.gz\")\n",
    "        print(f\"Đã lưu: {output_path}\")\n",
    "\n",
    "        # 4. Rotate\n",
    "        angle = np.random.uniform(-10, 10)  # Xoay ngẫu nhiên ±10 độ\n",
    "        rotated_data = rotate(data, angle, axes=(0, 1), reshape=False, order=3)\n",
    "        normalized_rotated = resize_and_normalize_fMRI(rotated_data, affine, target_shape)\n",
    "        output_path = os.path.join(output_dir, f\"{base_name}_rotated.nii.gz\")\n",
    "        nib.save(nib.Nifti1Image(normalized_rotated, affine), output_path)\n",
    "        output_files.append(f\"{base_name}_rotated.nii.gz\")\n",
    "        print(f\"Đã lưu: {output_path}\")\n",
    "\n",
    "        return output_files\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi xử lý {input_path}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Đường dẫn đầu vào và đầu ra\n",
    "input_dir = \"./data\"\n",
    "output_dir = \"./data_augmented\"\n",
    "\n",
    "# Tạo thư mục đầu ra\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Lấy danh sách file NIfTI\n",
    "files = [f for f in os.listdir(input_dir) if f.endswith((\".nii\", \".nii.gz\"))]\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    input_path = os.path.join(input_dir, file)\n",
    "    output_files = augment_fMRI(input_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c7b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csv file\n",
    "def create_augmented_csv(original_csv_path, output_dir, output_csv_path):\n",
    "    df = pd.read_csv(original_csv_path)\n",
    "    if not all(col in df.columns for col in ['filename', 'image_id', 'subject_id', 'label']):\n",
    "        raise ValueError(\"CSV phải chứa các cột 'filename', 'image_id', 'subject_id', 'label'\")\n",
    "    new_rows = []\n",
    "    \n",
    "    for file in os.listdir(output_dir):\n",
    "        if file.endswith((\".nii\", \".nii.gz\")):\n",
    "            base_name = file.replace('_original', '').replace('_zoomed', '').replace('_shifted', '').replace('_rotated', '').replace('.nii', '').replace('.nii.gz', '').replace('_rest', '').replace('_filtered', '')\n",
    "            base_name = os.path.splitext(base_name)[0]\n",
    "            image_id = base_name.split('-')\n",
    "            if len(image_id) == 1:\n",
    "                image_id = image_id[0]\n",
    "            else:\n",
    "                image_id = image_id[1]\n",
    "            if image_id[0] == \"I\":\n",
    "                image_id = image_id.replace('I', '')\n",
    "            matching_rows = df[df[\"image_id\"] == int(image_id)]\n",
    "            if not matching_rows.empty:\n",
    "                row = matching_rows.iloc[0]\n",
    "                new_rows.append({\n",
    "                    'filename': file,\n",
    "                    'image_id': image_id,\n",
    "                    'subject_id': row['subject_id'],\n",
    "                    'label': row['label']\n",
    "                })\n",
    "\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    new_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Đã tạo CSV mới: {output_csv_path}\")\n",
    "\n",
    "output_dir = \"./data_augmented\"\n",
    "output_csv_path = \"./labels_augmented.csv\"\n",
    "original_csv_path = \"./final_nii_metadata.csv\"\n",
    "\n",
    "create_augmented_csv(original_csv_path, output_dir, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2fb99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã tạo: train.csv (1272), val.csv (70), test.csv (50)\n"
     ]
    }
   ],
   "source": [
    "# Split train_test_val\n",
    "def split_dataset(csv_path, output_dir, train_size=1272, val_size=70, test_size=50):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Lấy các file gốc (không augment) cho val/test\n",
    "    original_files = df[df['filename'].str.contains('_original')]\n",
    "    \n",
    "    # Chia val/test từ file gốc\n",
    "    val_test_df = original_files.sample(n=val_size + test_size, random_state=42)\n",
    "    val_df, test_df = train_test_split(val_test_df, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Tập train bao gồm tất cả file (gốc + augment), trừ các file trong val/test\n",
    "    train_df = df[~df['filename'].isin(val_test_df['filename'])]\n",
    "    \n",
    "    # Lấy đúng số mẫu train\n",
    "    train_df = train_df.sample(n=train_size, random_state=42)\n",
    "    \n",
    "    # Lưu các file CSV\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    train_df.to_csv(os.path.join(output_dir, 'train.csv'), index=False)\n",
    "    val_df.to_csv(os.path.join(output_dir, 'val.csv'), index=False)\n",
    "    test_df.to_csv(os.path.join(output_dir, 'test.csv'), index=False)\n",
    "    print(f\"Đã tạo: train.csv ({len(train_df)}), val.csv ({len(val_df)}), test.csv ({len(test_df)})\")\n",
    "\n",
    "# Chạy chia dữ liệu\n",
    "output_dir = \"./data_splits\"\n",
    "split_dataset(\"./labels_augmented.csv\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "836d1a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_intensity(img_tensor, normalization=\"mean\"):\n",
    "    if normalization == \"mean\":\n",
    "        mask = img_tensor.ne(0.0)\n",
    "        desired = img_tensor[mask]\n",
    "        mean_val, std_val = desired.mean(), desired.std()\n",
    "        img_tensor = (img_tensor - mean_val) / std_val\n",
    "    elif normalization == \"max\":\n",
    "        MAX, MIN = img_tensor.max(), img_tensor.min()\n",
    "        img_tensor = (img_tensor - MIN) / (MAX - MIN)\n",
    "    return img_tensor\n",
    "\n",
    "class loader(Dataset):\n",
    "    def __init__(self, csv_path, data_dir, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.label_map = {1: 0, 2: 1, 3: 2}  # Ánh xạ: 1->0 (normal), 2->1 (MCI), 3->2 (AD)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filename = self.df.iloc[index]['filename']\n",
    "        label = self.label_map[self.df.iloc[index]['label']]\n",
    "        img_path = os.path.join(self.data_dir, filename)\n",
    "        img = nib.load(img_path).get_fdata()\n",
    "        \n",
    "        if img.shape != (64, 64, 48):\n",
    "            zoom_factors = [t / s for t, s in zip((64, 64, 48), img.shape)]\n",
    "            img = zoom(img, zoom_factors, order=3)\n",
    "        \n",
    "        img_tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0)\n",
    "        img_tensor = normalize_intensity(img_tensor)\n",
    "        \n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img_tensor)\n",
    "        \n",
    "        return img_tensor, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# 4. Mô hình CNN\n",
    "def compute_output_size(i, K, P, S):\n",
    "    return int(((i - K + 2*P) / S) + 1)\n",
    "\n",
    "class CNN_8CL_B:\n",
    "    def __init__(self):\n",
    "        self.input_dim = [64, 64, 48]\n",
    "        self.out_channels = [8, 8, 16, 16, 32, 32, 64, 64]\n",
    "        self.in_channels = [1] + self.out_channels[:-1]\n",
    "        self.n_conv = len(self.out_channels)\n",
    "        self.kernels = [(3, 3, 3)] * self.n_conv\n",
    "        self.pooling = [(4, 4, 4), (0, 0, 0), (3, 3, 3), (0, 0, 0), (2, 2, 2),\n",
    "                        (0, 0, 0), (2, 2, 2), (0, 0, 0)]\n",
    "        for i in range(self.n_conv):\n",
    "            for d in range(3):\n",
    "                if self.pooling[i][d] != 0:\n",
    "                    self.input_dim[d] = compute_output_size(self.input_dim[d], self.pooling[i][d], 0, self.pooling[i][d])\n",
    "        out = self.input_dim[0] * self.input_dim[1] * self.input_dim[2]\n",
    "        self.fweights = [self.out_channels[-1] * out, 3]  # [64, 3] cho 3 lớp\n",
    "        self.dropout = 0.0\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, param):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding = nn.ModuleList()\n",
    "        for i in range(param.n_conv):\n",
    "            pad = tuple([int((k-1)/2) for k in param.kernels[i]])\n",
    "            if param.pooling[i] != (0, 0, 0):\n",
    "                self.embedding.append(nn.Sequential(\n",
    "                    nn.Conv3d(param.in_channels[i], param.out_channels[i], param.kernels[i], stride=(1, 1, 1), padding=pad, bias=False),\n",
    "                    nn.BatchNorm3d(param.out_channels[i]),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.MaxPool3d(param.pooling[i], stride=param.pooling[i])))\n",
    "            else:\n",
    "                self.embedding.append(nn.Sequential(\n",
    "                    nn.Conv3d(param.in_channels[i], param.out_channels[i], param.kernels[i], stride=(1, 1, 1), padding=pad, bias=False),\n",
    "                    nn.BatchNorm3d(param.out_channels[i]),\n",
    "                    nn.ReLU(inplace=True)))\n",
    "        self.ReLU = nn.ReLU(inplace=True)\n",
    "        self.Dropout = nn.Dropout(p=param.dropout)\n",
    "        self.f = nn.ModuleList()\n",
    "        for i in range(len(param.fweights)-1):\n",
    "            self.f.append(nn.Linear(param.fweights[i], param.fweights[i+1]))\n",
    "    \n",
    "    def forward(self, x, return_conv=False):\n",
    "        out = self.embedding[0](x)\n",
    "        if return_conv:\n",
    "            all_layers = [out]\n",
    "        for i in range(1, len(self.embedding)):\n",
    "            out = self.embedding[i](out)\n",
    "            if return_conv:\n",
    "                all_layers.append(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        for fc in self.f[:-1]:\n",
    "            out = fc(out)\n",
    "            out = self.ReLU(out)\n",
    "            out = self.Dropout(out)\n",
    "        out = self.f[-1](out)\n",
    "        if return_conv:\n",
    "            return F.softmax(out, dim=1), all_layers\n",
    "        else:\n",
    "            return F.softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "95b9e016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã tạo: train.csv (1272), val.csv (70), test.csv (50)\n",
      "[========================================] 40/40 [100%] in 30.7s (1.30/s) \n",
      "Epoch 1/200, Train Loss: 1.0591, Val Acc: 0.3286\n",
      "[========================================] 40/40 [100%] in 29.0s (1.38/s) \n",
      "Epoch 2/200, Train Loss: 0.9816, Val Acc: 0.4000\n",
      "[========================================] 40/40 [100%] in 28.9s (1.39/s) \n",
      "Epoch 3/200, Train Loss: 0.9125, Val Acc: 0.3429\n",
      "[========================================] 40/40 [100%] in 28.7s (1.39/s) \n",
      "Epoch 4/200, Train Loss: 0.8503, Val Acc: 0.7286\n",
      "[========================================] 40/40 [100%] in 28.9s (1.38/s) \n",
      "Epoch 5/200, Train Loss: 0.7846, Val Acc: 0.6857\n",
      "[========================================] 40/40 [100%] in 28.7s (1.40/s) \n",
      "Epoch 6/200, Train Loss: 0.7696, Val Acc: 0.8143\n",
      "[========================================] 40/40 [100%] in 28.7s (1.39/s) \n",
      "Epoch 7/200, Train Loss: 0.7314, Val Acc: 0.8286\n",
      "[========================================] 40/40 [100%] in 28.7s (1.39/s) \n",
      "Epoch 8/200, Train Loss: 0.7057, Val Acc: 0.4429\n",
      "[========================================] 40/40 [100%] in 28.9s (1.39/s) \n",
      "Epoch 9/200, Train Loss: 0.6860, Val Acc: 0.4714\n",
      "[========================================] 40/40 [100%] in 28.7s (1.39/s) \n",
      "Epoch 10/200, Train Loss: 0.6853, Val Acc: 0.7714\n",
      "[========================================] 40/40 [100%] in 28.5s (1.40/s) \n",
      "Epoch 11/200, Train Loss: 0.6713, Val Acc: 0.5143\n",
      "[========================================] 40/40 [100%] in 28.5s (1.40/s) \n",
      "Epoch 12/200, Train Loss: 0.6592, Val Acc: 0.8714\n",
      "[========================================] 40/40 [100%] in 28.8s (1.39/s) \n",
      "Epoch 13/200, Train Loss: 0.6358, Val Acc: 0.7000\n",
      "[========================================] 40/40 [100%] in 29.0s (1.38/s) \n",
      "Epoch 14/200, Train Loss: 0.6207, Val Acc: 0.7714\n",
      "[========================================] 40/40 [100%] in 28.5s (1.40/s) \n",
      "Epoch 15/200, Train Loss: 0.6139, Val Acc: 0.6000\n",
      "[========================================] 40/40 [100%] in 28.9s (1.38/s) \n",
      "Epoch 16/200, Train Loss: 0.6096, Val Acc: 0.4429\n",
      "[========================================] 40/40 [100%] in 28.9s (1.38/s) \n",
      "Epoch 17/200, Train Loss: 0.5954, Val Acc: 0.8857\n",
      "[========================================] 40/40 [100%] in 28.3s (1.41/s) \n",
      "Epoch 18/200, Train Loss: 0.5930, Val Acc: 0.9857\n",
      "[========================================] 40/40 [100%] in 28.6s (1.40/s) \n",
      "Epoch 19/200, Train Loss: 0.5965, Val Acc: 1.0000\n",
      "[========================================] 40/40 [100%] in 28.3s (1.41/s) \n",
      "Epoch 20/200, Train Loss: 0.5925, Val Acc: 0.9714\n",
      "[========================================] 40/40 [100%] in 28.4s (1.41/s) \n",
      "Epoch 21/200, Train Loss: 0.5864, Val Acc: 0.8000\n",
      "[========================================] 40/40 [100%] in 28.3s (1.42/s) \n",
      "Epoch 22/200, Train Loss: 0.5861, Val Acc: 0.9286\n",
      "[========================================] 40/40 [100%] in 28.3s (1.41/s) \n",
      "Epoch 23/200, Train Loss: 0.5993, Val Acc: 0.9714\n",
      "[========================================] 40/40 [100%] in 28.1s (1.42/s) \n",
      "Epoch 24/200, Train Loss: 0.5896, Val Acc: 0.9857\n",
      "[========================================] 40/40 [100%] in 27.6s (1.45/s) \n",
      "Epoch 25/200, Train Loss: 0.5863, Val Acc: 0.9286\n",
      "[========================================] 40/40 [100%] in 27.7s (1.44/s) \n",
      "Epoch 26/200, Train Loss: 0.5863, Val Acc: 1.0000\n",
      "[========================================] 40/40 [100%] in 28.6s (1.40/s) \n",
      "Epoch 27/200, Train Loss: 0.5838, Val Acc: 1.0000\n",
      "[========================================] 40/40 [100%] in 28.4s (1.41/s) \n",
      "Epoch 28/200, Train Loss: 0.5932, Val Acc: 1.0000\n",
      "[========================================] 40/40 [100%] in 28.4s (1.41/s) \n",
      "Epoch 29/200, Train Loss: 0.5800, Val Acc: 0.9571\n",
      "[========================================] 40/40 [100%] in 28.4s (1.41/s) \n",
      "Epoch 30/200, Train Loss: 0.5829, Val Acc: 0.8571\n",
      "[========================================] 40/40 [100%] in 28.3s (1.41/s) \n",
      "Epoch 31/200, Train Loss: 0.5912, Val Acc: 0.9429\n",
      "[========================================] 40/40 [100%] in 29.1s (1.38/s) \n",
      "Epoch 32/200, Train Loss: 0.5931, Val Acc: 0.9000\n",
      "[========================================] 40/40 [100%] in 29.3s (1.36/s) \n",
      "Epoch 33/200, Train Loss: 0.5942, Val Acc: 0.9714\n",
      "[========================================] 40/40 [100%] in 28.5s (1.41/s) \n",
      "Epoch 34/200, Train Loss: 0.5859, Val Acc: 0.8714\n",
      "[========================================] 40/40 [100%] in 28.4s (1.41/s) \n",
      "Epoch 35/200, Train Loss: 0.5822, Val Acc: 0.7143\n",
      "[========================================] 40/40 [100%] in 27.6s (1.45/s) \n",
      "Epoch 36/200, Train Loss: 0.5835, Val Acc: 0.9000\n",
      "[========================================] 40/40 [100%] in 28.2s (1.42/s) \n",
      "Epoch 37/200, Train Loss: 0.6042, Val Acc: 0.8286\n",
      "[========================================] 40/40 [100%] in 27.8s (1.44/s) \n",
      "Epoch 38/200, Train Loss: 0.6186, Val Acc: 0.5857\n",
      "[========================================] 40/40 [100%] in 27.3s (1.47/s) \n",
      "Epoch 39/200, Train Loss: 0.5960, Val Acc: 0.8571\n",
      "Early stopping!\n",
      "\n",
      "Model prediction. Total number of steps: 2\n",
      "[========================================] 2/2 [100%] in 0.7s (2.89/s) \n",
      "Test Accuracy: 0.8600\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, device, max_epochs=200, max_errors=20, lr=0.001, weight_decay=0.01):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience = max_errors\n",
    "    errors = 0\n",
    "    history = {'train_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        with alive_bar(len(train_loader), bar='classic', spinner='arrow') as bar:\n",
    "            for x, y in train_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                bar()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_preds, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                outputs = model(x)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(y.cpu().numpy())\n",
    "        \n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{max_epochs}, Train Loss: {train_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            errors = 0\n",
    "        else:\n",
    "            errors += 1\n",
    "            if errors >= max_errors:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "    \n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, test_loader, device, saver_path):\n",
    "    model.eval()\n",
    "    labels, prob0, prob1, prob2, preds = predict(model, test_loader, device, return_prob=True)\n",
    "    \n",
    "    test_acc = accuracy_score(labels, preds)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    plot_confusion_matrix(labels, preds, saver_path)\n",
    "    \n",
    "    # ROC Curve cho từng lớp\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    for i, cls in enumerate(['normal', 'MCI', 'AD']):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels == i, [prob0, prob1, prob2][i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    plot_auc_curve(fpr, tpr, roc_auc, saver_path)\n",
    "    \n",
    "    # Classification Report\n",
    "    report = pd.DataFrame({\n",
    "        'Precision': [0.0, 0.0, 0.0],\n",
    "        'Recall': [0.0, 0.0, 0.0],\n",
    "        'F1-Score': [0.0, 0.0, 0.0]\n",
    "    }, index=['normal', 'MCI', 'AD'])\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    for i, cls in enumerate(['normal', 'MCI', 'AD']):\n",
    "        precision = cm[i, i] / cm[:, i].sum() if cm[:, i].sum() > 0 else 0\n",
    "        recall = cm[i, i] / cm[i, :].sum() if cm[i, :].sum() > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        report.loc[cls] = [precision, recall, f1]\n",
    "    plot_complete_report(report, saver_path, labels=['normal', 'MCI', 'AD'])\n",
    "\n",
    "def predict(net, data_loader, device, return_prob=True):\n",
    "    predictions, labels = torch.tensor([]), torch.tensor([])\n",
    "    all_prob0, all_prob1, all_prob2 = torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
    "    print('\\nModel prediction. Total number of steps:', len(data_loader))\n",
    "    with alive_bar(len(data_loader), bar='classic', spinner='arrow') as bar:\n",
    "        for _, (x, y) in enumerate(data_loader):\n",
    "            x = x.to(device)\n",
    "            output = net(x).detach().cpu()\n",
    "            if return_prob:\n",
    "                prob0 = output[:, 0]\n",
    "                prob1 = output[:, 1]\n",
    "                prob2 = output[:, 2]\n",
    "                all_prob0 = torch.cat((all_prob0, prob0), dim=0)\n",
    "                all_prob1 = torch.cat((all_prob1, prob1), dim=0)\n",
    "                all_prob2 = torch.cat((all_prob2, prob2), dim=0)\n",
    "            y_pred = torch.argmax(output, dim=1)\n",
    "            predictions = torch.cat((predictions, y_pred), dim=0)\n",
    "            labels = torch.cat((labels, y), dim=0)\n",
    "            bar()\n",
    "        if return_prob:\n",
    "            return labels.numpy(), all_prob0.numpy(), all_prob1.numpy(), all_prob2.numpy(), predictions.numpy()\n",
    "        else:\n",
    "            return labels.numpy(), predictions.numpy()\n",
    "\n",
    "def plot_complete_report(data, saver_path, labels=None):\n",
    "    if labels:\n",
    "        data['Class'] = labels\n",
    "        data = data.set_index('Class')\n",
    "    plt.figure(figsize=(10,3.5))\n",
    "    ax = sns.heatmap(data, annot=data, fmt='.2f', square=True, cmap='Blues')\n",
    "    ax.set(ylabel='Classes')\n",
    "    plt.title('Classification Report')\n",
    "    plt.savefig(f'{saver_path}/Evaluation.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_auc_curve(fpr, tpr, roc_auc, saver_path):\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    for i, cls in enumerate(['normal', 'MCI', 'AD']):\n",
    "        plt.plot(fpr[i], tpr[i], label=f\"AUC {cls}={roc_auc[i]:.4f}\")\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.title('AUC curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(f'{saver_path}/AUC.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_test, y_pred, saver_path):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=['normal', 'MCI', 'AD'])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.savefig(f'{saver_path}/confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Chia dữ liệu\n",
    "split_dataset(output_csv_path, \"./data_splits\")\n",
    "\n",
    "# Đào tạo và đánh giá\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN(CNN_8CL_B()).to(device)\n",
    "train_loader = DataLoader(loader(\"./data_splits/train.csv\", \"./data_augmented\"), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(loader(\"./data_splits/val.csv\", \"./data_augmented\"), batch_size=32)\n",
    "test_loader = DataLoader(loader(\"./data_splits/test.csv\", \"./data_augmented\"), batch_size=32)\n",
    "\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "history = train_model(model, train_loader, val_loader, device)\n",
    "evaluate_model(model, test_loader, device, \"./results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
